{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HpDiniz/Leitor-de-PDF/blob/main/Projeto_de_Pesquisa_Mestrado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z52BuxHJdJy-"
      },
      "outputs": [],
      "source": [
        "janela_treino = 1\n",
        "tipo_interesse = \"Tijolo\" # Papel, Tijolo ou Hibrido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiywEkwhdHss"
      },
      "source": [
        "# 0. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFZouOfDMgM9"
      },
      "outputs": [],
      "source": [
        "import warnings;\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I3vDXARwdDR",
        "outputId": "d0257972-1d19-4597-f81e-69eae8b5a826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.9/255.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.5/210.5 KB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.5/147.5 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.9/575.9 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install pystan --quiet\n",
        "!pip install statsmodels --quiet\n",
        "!pip install xgboost==1.6.2 --quiet\n",
        "!pip install pmdarima --quiet\n",
        "!pip install mysqlclient --quiet\n",
        "!pip install psycopg2-binary==2.8.6 --quiet\n",
        "!pip install mlflow --quiet\n",
        "!pip install pyngrok --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDko7zAeR0zL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import bs4\n",
        "import json\n",
        "import pickle\n",
        "import requests\n",
        "import datetime\n",
        "import dateutil\n",
        "import itertools\n",
        "import statistics\n",
        "\n",
        "from datetime import date\n",
        "from prophet import Prophet\n",
        "from bs4 import BeautifulSoup\n",
        "from xgboost import XGBRegressor\n",
        "from pmdarima import auto_arima\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Avaliando os resultados\n",
        "from numpy import sqrt\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ewy3adg8tqYQ"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "experiment_name = f'FII-{tipo_interesse}-{janela_treino}M-202301'\n",
        "\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = \"henrique.p.diniz\"\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = \"47df072ea2fe3bd50e27c06cf5eeb20e74460e50\"\n",
        "os.environ['MLFLOW_TRACKING_PROJECTNAME'] = \"Projeto-Pesquisa-Mestrado\"\n",
        "\n",
        "def get_experiments_result(experiment_name, sort_column = \"\"):\n",
        "\n",
        "    mlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n",
        "    mlflow_experiment = mlflow.set_experiment(experiment_name)\n",
        "    df = mlflow.search_runs([mlflow_experiment._experiment_id])\n",
        "\n",
        "    if(len(df) > 0):\n",
        "        if sort_column in df.columns:\n",
        "            return df.sort_values(by=sort_column, ascending=True)\n",
        "        else:\n",
        "            return df\n",
        "\n",
        "    return df\n",
        "\n",
        "# mlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n",
        "# mlflow_experiment = mlflow.set_experiment(experiment_name)\n",
        "# df_mlflow = mlflow.search_runs([mlflow_experiment._experiment_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rsm__ClwdDS"
      },
      "source": [
        "# 1. Read in Data and Process Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyp-wBpT4wr-"
      },
      "outputs": [],
      "source": [
        "first_day = pd.to_datetime('today').replace(day=1,hour=0,minute=0,second=0,microsecond=0)\n",
        "this_month = \"2023-01\" #(first_day).strftime(\"%Y-%m\")\n",
        "last_month = \"2022-12\" #(first_day - relativedelta(months=2)).strftime(\"%Y-%m\")\n",
        "\n",
        "headers = {\n",
        "    'User-Agent':\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36'\n",
        "        ' (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\n",
        "}\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "engine = create_engine('postgresql://wzmywfei:yU9UYTEgfnTRQVkBF_oBcSCwLJtzmd5r@kesavan.db.elephantsql.com/wzmywfei', echo=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsgUw0m7Xw0q"
      },
      "outputs": [],
      "source": [
        "def converteData(datas, monthYearOnly):\n",
        "\n",
        "    new_array = []\n",
        "    meses = [\"Janeiro\",\"Fevereiro\",\"Março\",\"Abril\",\"Maio\",\"Junho\",\"Julho\",\"Agosto\",\"Setembro\",\"Outubro\",\"Novembro\",\"Dezembro\"]\n",
        "\n",
        "    for data in datas:\n",
        "\n",
        "        item = data.split(\"/\")\n",
        "        mes = str(meses.index(item[0])+1)\n",
        "        mes = (\"0\" + mes)[len(mes)-1:len(mes)+1]\n",
        "\n",
        "        new_date = item[1] + \"-\" + mes\n",
        "\n",
        "        if not monthYearOnly:\n",
        "            new_date = new_date + \"-01 00:00:00\"\n",
        "        \n",
        "        new_array.append(new_date)\n",
        "        \n",
        "    return new_array\n",
        "\n",
        "def obtem_datas_faltantes(df, date_colun):\n",
        "\n",
        "    datas_faltantes = []\n",
        "    start_date = df[date_colun].min()\n",
        "    end_date = df[date_colun].max()\n",
        "\n",
        "    while(start_date < end_date):\n",
        "        date = str(start_date)[0:10]\n",
        "        df_aux = df[df[date_colun] == date]\n",
        "\n",
        "        if(len(df_aux) < 1):\n",
        "            datas_faltantes.append(date)\n",
        "\n",
        "        start_date = (start_date + relativedelta(days=1))\n",
        "\n",
        "    return datas_faltantes\n",
        "\n",
        "def obtem_dados_mercado(indice):\n",
        "\n",
        "    indice = indice.lower()\n",
        "\n",
        "    if indice == \"igpm\":\n",
        "        indice = \"igp-m\"\n",
        "\n",
        "    response = requests.get('https://www.dadosdemercado.com.br/economia/' + indice, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        df_igpm = pd.read_html(response.content, encoding='utf-8')[0]\n",
        "\n",
        "    anos = list(df_igpm.iloc[:, 0].values)\n",
        "\n",
        "    timestamp = []\n",
        "    values = []\n",
        "\n",
        "    for i in range(len(anos)):\n",
        "        for m in range(12, 0, -1):\n",
        "            taxa = str(list(df_igpm.iloc[:, m].values)[i])\n",
        "            if taxa != '--':\n",
        "                mes = str(m) if m > 9 else \"0\" + str(m)\n",
        "                timestamp.append(str(anos[i]) + \"-\" + mes)\n",
        "                values.append(round(float(taxa.replace(\"%\",\"\").replace(\",\",\".\")), 2))\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_tax = pd.DataFrame({\n",
        "        'Timestamp': timestamp,\n",
        "        'Value': values\n",
        "    })\n",
        "\n",
        "    df_tax['Value'] = pd.to_numeric(df_tax['Value'], downcast=\"float\")\n",
        "\n",
        "    return df_tax.replace(0, 0.01) \n",
        "\n",
        "def get_all_funds():\n",
        "\n",
        "    response = requests.get('https://www.fundsexplorer.com.br/ranking', headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        df = pd.read_html(response.content, encoding='utf-8')[0]\n",
        "\n",
        "    idx = df[df['Setor'].isna()].index\n",
        "    df.drop(idx, inplace=True)\n",
        "\n",
        "    df_funds = df.rename(columns={'Códigodo fundo': 'Ticker'})\n",
        "\n",
        "    col_categorical = ['Ticker','Setor']\n",
        "    df_funds[col_categorical] = df_funds[col_categorical].astype('category')\n",
        "\n",
        "    df_funds.sort_values('Ticker', inplace=True)\n",
        "\n",
        "    df_funds = df_funds.drop_duplicates(subset=['Ticker']).replace('Títulos e Valores Mobiliários','Títulos e Val. Mob.')\n",
        "\n",
        "    df_funds = df_funds[['Ticker','Setor','QuantidadeAtivos']].reset_index(drop=True)\n",
        "\n",
        "    return df_funds\n",
        "\n",
        "def get_close(fund, years):\n",
        "\n",
        "    df_close = pd.DataFrame()\n",
        "\n",
        "    end_date = (first_day).strftime(\"%d-%m-%Y\")\n",
        "    start_date = \"01-01-\" + str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) \n",
        "    \n",
        "    response = requests.get('https://fii-api.infomoney.com.br/api/v1/fii/cotacao/historico/grafico?Ticker='+fund+'&DataInicio='+start_date+'&DataFim='+end_date, headers=headers)\n",
        "\n",
        "    if not str(response.content) == \"b''\":\n",
        "\n",
        "        json_response = json.loads(response.content)\n",
        "\n",
        "        if 'errors' in json_response:\n",
        "            print(str(json_response['errors']))\n",
        "        else:\n",
        "            df_close = pd.read_json(json.dumps(json_response['dataValor']))\n",
        "\n",
        "            df_close['Ticker'] = fund\n",
        "            df_close['Ticker'] = df_close['Ticker'].astype('category')\n",
        "\n",
        "            df_close.rename(columns={'valor': 'Close'}, inplace = True)\n",
        "\n",
        "            df_close['Datetime'] = pd.to_datetime(df_close['data'], format='%d-%m-%YT%H:%M:%S')\n",
        "\n",
        "            df_close.drop(columns={'data'}, inplace = True)\n",
        "        \n",
        "    return df_close.replace(0, 0.01) \n",
        "\n",
        "def get_dividends(fund, years):\n",
        "\n",
        "    min_date = str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) + \"-01\"\n",
        "\n",
        "    response = requests.get('https://www.fundsexplorer.com.br/funds/' + fund, headers=headers)\n",
        "\n",
        "    soup = bs4.BeautifulSoup(response.content, \"html\")\n",
        "    div = soup.find(\"div\", {\"id\": \"dividends-chart-wrapper\"})\n",
        "\n",
        "    labels = re.findall('\"labels\":\\[.*?\\]', str(div))\n",
        "    dividends = re.findall('\"data\":\\[.*?\\]', str(div))\n",
        "\n",
        "    dividends = json.loads(\"{\" + dividends[0] + \"}\")['data']\n",
        "    labels = json.loads(\"{\" + labels[0] + \"}\")['labels']\n",
        "\n",
        "    dates = converteData(labels, True)\n",
        "\n",
        "    result = []\n",
        "    if len(dates) > 0 and len(dates) == len(dividends):\n",
        "        for i in range(len(dates)):\n",
        "            if dates[i] >= min_date:\n",
        "                result.append({\n",
        "                    \"Ticker\": fund,\n",
        "                    \"Datetime\": dates[i],\n",
        "                    \"Dividends\": round(dividends[i],2)\n",
        "                })\n",
        "\n",
        "    df_dividends = pd.DataFrame(result)\n",
        "\n",
        "    return df_dividends.replace(0, 0.01) \n",
        "\n",
        "def get_adress(fundo):\n",
        "\n",
        "    api_url = \"https://fii-api.infomoney.com.br/api/v1/propertie/\" + fundo\n",
        "    response = requests.get(api_url)\n",
        "    data = []\n",
        "\n",
        "    if '{' in str(response.content):\n",
        "\n",
        "        response = response.json()\n",
        "\n",
        "        for item in response[\"property\"]:\n",
        "\n",
        "            row = {\n",
        "                \"Ticker\": fundo,\n",
        "                \"Tipo\": item[\"type\"],\n",
        "                \"Nome\": item[\"name\"],\n",
        "                \"DataCompra\": item[\"datePurchase\"],\n",
        "                \"ValorAreaBrutaLocavel\": item[\"valueGrossLeasableArea\"],\n",
        "                \"Estado\": item[\"state\"],\n",
        "                \"Cidade\": item[\"city\"],\n",
        "                \"Endereco\": item[\"address\"],\n",
        "                \"GoogleMapsLink\": item[\"googleMapsLink\"],\n",
        "                \"PercentualPartic\": item[\"percentagePartic\"],\n",
        "                \"PecentualVacancia\": item[\"percentVacancy\"],\n",
        "                \"PercentualInadimplencia90Dias\": item[\"percent90DayDeliquency\"],\n",
        "                \"PercentualFii\": item[\"percentFii\"],\n",
        "                \"Latitude\": float(\"NaN\"),\n",
        "                \"Longitude\": float(\"NaN\")\n",
        "            }\n",
        "\n",
        "            cordinates = re.findall(\"(?<=@)[-]*[\\d.]*,-[\\d.]*\", item['googleMapsLink'])\n",
        "\n",
        "            if(len(cordinates) > 0):\n",
        "                cordinates = cordinates[0].split(\",\")\n",
        "                row[\"Latitude\"], row[\"Longitude\"] = float(cordinates[0]), float(cordinates[1])\n",
        "            else:\n",
        "                \n",
        "                adress_url = (\"https://www.google.com/maps/place/\" + item[\"address\"] + \",\" + item[\"city\"] + \"-\" + item[\"state\"]).replace(\" \", \"%20\")\n",
        "\n",
        "                response = requests.get(adress_url)\n",
        "\n",
        "                cordinates = re.findall(\"(?<=@)[-]*[\\d.]*,-[\\d.]*\", str(response.content))\n",
        "\n",
        "                if(len(cordinates) > 0):\n",
        "                    print(\"Endereço não encontrado, obtendo Latitude e Longitude aproximada...\")\n",
        "                    cordinates = cordinates[0].split(\",\")\n",
        "                    row[\"Latitude\"], row[\"Longitude\"] = float(cordinates[0]), float(cordinates[1])\n",
        "                else:\n",
        "                    print(\"Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\")\n",
        "\n",
        "            data.append(row)\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def get_month_close(df_close, date):\n",
        "\n",
        "    year = int(date.split('-')[0])\n",
        "    month = int(date.split('-')[1])\n",
        "\n",
        "    start_date = pd.to_datetime('today').replace(year=year, month= month, day=1,hour=0,minute=0,second=0,microsecond=0)\n",
        "    end_date = (start_date + relativedelta(months=1))\n",
        "\n",
        "    df_aux = df_close.copy()\n",
        "\n",
        "    #print(\"Procurando fechamento entre: \" + str(start_date) + \" e \" + str(end_date))\n",
        "\n",
        "    df_aux = df_aux[df_aux['Datetime'] >= start_date]\n",
        "    df_aux = df_aux[df_aux['Datetime'] < end_date]\n",
        "\n",
        "    if len(df_aux) > 0:\n",
        "        return df_aux.values[-1][0]\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def has_missing_data(df_history):\n",
        "\n",
        "    min = str(df_history['Datetime'].min())\n",
        "    max = str(df_history['Datetime'].max())\n",
        "\n",
        "    year = int(max.split('-')[0])\n",
        "    month = int(max.split('-')[1])\n",
        "\n",
        "    start_date = pd.to_datetime('today').replace(year=year, month=month, day=1,hour=0,minute=0,second=0,microsecond=0)\n",
        "\n",
        "    while str(start_date.strftime(\"%Y-%m\")) != min:\n",
        "\n",
        "        if not str(start_date.strftime(\"%Y-%m\")) in list(df_history['Datetime']):\n",
        "            return True\n",
        "\n",
        "        start_date = (start_date - relativedelta(months=1))\n",
        "\n",
        "    return False\n",
        "\n",
        "def get_history(fund, years):\n",
        "\n",
        "    df_close = get_close(fund, years)\n",
        "    df_dividends = get_dividends(fund, years)\n",
        "\n",
        "    df_history = df_dividends.copy()\n",
        "\n",
        "    if len(df_history) > 0 and len(df_close) > 0:\n",
        "\n",
        "        new_df = []\n",
        "        for index, row in df_history.iterrows():\n",
        "\n",
        "            #print(\"Procurando 'Close' de: \" + row['Datetime'])\n",
        "            row['Dividends'] = round(row['Dividends'],2)\n",
        "            row['Close'] = get_month_close(df_close, row['Datetime'])\n",
        "            new_df.append(row)\n",
        "\n",
        "        df_history = pd.DataFrame(new_df)\n",
        "\n",
        "        datas = list(df_history['Datetime'])\n",
        "\n",
        "        if has_missing_data(df_history):\n",
        "            print(\"FII \" + fund + \" será removido por estar com dados faltantes.\")\n",
        "            df_history = pd.DataFrame()\n",
        "    \n",
        "    return df_history\n",
        "\n",
        "def process_daily_history(df_history, years):\n",
        "\n",
        "    # Cria um array de índices\n",
        "    indices = ['Selic','IPCA','IGPM']\n",
        "\n",
        "    # Obtém o histórico de índices\n",
        "    df_indices = {}\n",
        "    for indice in indices:\n",
        "        df_indices[indice] = obtem_dados_mercado(indice)\n",
        "\n",
        "    # Obtém o histórico do IFIX\n",
        "    df_ifix = get_ifix(2)\n",
        "\n",
        "    # Cria o histórico diário\n",
        "    df_history_daily = pd.DataFrame()\n",
        "\n",
        "    for fund in df_history['Ticker'].unique():\n",
        "\n",
        "        print(\"Coletando informações de \" + fund + \"...\")\n",
        "\n",
        "        df_close = get_close(fund, years)\n",
        "\n",
        "        df_close[\"Datetime\"] = pd.to_datetime(df_close[\"Datetime\"], format=\"%Y-%m-%d\")\n",
        "\n",
        "        # Preenche os índices mensais\n",
        "        meses_percorridos = []\n",
        "\n",
        "        for index, row in df_close.iterrows():\n",
        "            \n",
        "            data_mes = str(row['Datetime'])[0:7]\n",
        "            df_aux = df_history[(df_history['Datetime'] == data_mes) & (df_history['Ticker'] == fund)]\n",
        "\n",
        "            if len(df_aux) < 1 or data_mes in meses_percorridos:\n",
        "                continue\n",
        "\n",
        "            meses_percorridos.append(data_mes)\n",
        "            df_close.loc[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes)), \"Dividends\"] = float(df_aux['Dividends'].values[0])\n",
        "            \n",
        "            for indice in indices:\n",
        "                df_aux = df_indices[indice][df_indices[indice]['Timestamp'] == data_mes]\n",
        "                df_close.loc[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes)), indice] = float(df_aux['Value'].values[0])\n",
        "            \n",
        "            df_history_daily = df_history_daily.append(df_close[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes))])\n",
        "\n",
        "    # Preenche o IFIX em todas as datas do histórico diário\n",
        "    datas_percorridos = []\n",
        "    for index, row in df_history_daily.iterrows():\n",
        "        \n",
        "        data = str(row[\"Datetime\"])[0:10]\n",
        "\n",
        "        print(\"Preenchendo IFIX em \" + data + \"...\")\n",
        "\n",
        "        if data not in datas_percorridos:\n",
        "\n",
        "            df_aux = df_ifix[df_ifix['Datetime'] == data]\n",
        "\n",
        "            if(len(df_aux) > 0):\n",
        "\n",
        "                df_history_daily.loc[(df_history_daily[\"Datetime\"].dt.strftime(\"%Y-%m-%d\").eq(data)), \"IFIX\"] = float(df_aux['Close'].values[0])\n",
        "                datas_percorridos.append(data)\n",
        "\n",
        "    # Remove registros NaN\n",
        "    df_history_daily = df_history_daily.dropna()\n",
        "\n",
        "    return df_history_daily\n",
        "\n",
        "def preenche_historico_faltante(df_history_daily):\n",
        "\n",
        "    # Percorre todos os ativos do histórico\n",
        "    for ticker in df_history_daily['Ticker'].unique():\n",
        "\n",
        "        print(\"Adicionando dados faltantes de \" + ticker + \"...\")\n",
        "\n",
        "        # Obtém o histórico específico do ativo\n",
        "        df_aux = df_history_daily[df_history_daily['Ticker'] == ticker].copy()\n",
        "\n",
        "        # Obtém a menor e a maior data do histórico do ativo\n",
        "        start_date = pd.to_datetime(df_aux['Datetime']).min() + relativedelta(days=1)\n",
        "        end_date = pd.to_datetime(df_aux['Datetime']).max()\n",
        "\n",
        "        # Percorra todas as datas do intervalo\n",
        "        while(start_date < end_date):\n",
        "            \n",
        "            # Caso não haja algum registro no histórico para a data atual...\n",
        "            if (len(df_aux[df_aux['Datetime'].dt.strftime(\"%Y-%m-%d\").eq(str(start_date)[0:10])]) < 1):\n",
        "                \n",
        "                # Obtém a data de ontém\n",
        "                ontem = (start_date - relativedelta(days=1))\n",
        "\n",
        "                # Obtém os registros de ontém\n",
        "                df_ontem = df_history_daily[(df_history_daily['Ticker'] == ticker) & (df_history_daily['Datetime'].dt.strftime(\"%Y-%m-%d\").eq(str(ontem)[0:10]))]\n",
        "                \n",
        "                # Adiciona a data faltante no histórico\n",
        "                df_history_daily = df_history_daily.append(pd.DataFrame({\n",
        "                    \"Close\": df_ontem['Close'].values[0],\n",
        "                    \"Dividends\": df_ontem['Dividends'].values[0],\n",
        "                    \"Ticker\": [ticker],\n",
        "                    \"Datetime\": [start_date],\n",
        "                    \"Selic\": df_ontem['Selic'].values[0],\n",
        "                    \"IPCA\": df_ontem['IPCA'].values[0],\n",
        "                    \"IGPM\": df_ontem['IGPM'].values[0],\n",
        "                    \"IFIX\": df_ontem['IFIX'].values[0]\n",
        "                }))\n",
        "\n",
        "            # Incrementa a data de início\n",
        "            start_date = (start_date + relativedelta(days=1))\n",
        "\n",
        "    # Ordena todos os registros pelo Ticker e Data\n",
        "    df_history_daily.sort_values(by=['Ticker', 'Datetime'], inplace = True)\n",
        "    df_history_daily = df_history_daily.reset_index(drop = True)\n",
        "    return df_history_daily\n",
        "\n",
        "def process_history(df_funds, years):\n",
        "\n",
        "    df_adress = pd.DataFrame()\n",
        "    df_history = pd.DataFrame()\n",
        "    \n",
        "    # Percorre a lista de fundos para obter o histórico individual de cada um deles\n",
        "    for fund in df_funds['Ticker']:\n",
        "\n",
        "        print(\"Coletando informações de \" + fund + \"...\")\n",
        "\n",
        "        df_aux_1 = get_adress(fund)\n",
        "        df_aux_2 = get_history(fund, years)\n",
        "        \n",
        "        df_adress = df_adress.append(df_aux_1)\n",
        "        df_history = df_history.append(df_aux_2)\n",
        "\n",
        "        print(str(len(df_aux_2)) + \" dados de histórico e \" + str(len(df_aux_1)) + \" endereços foram encontrados.\")\n",
        "\n",
        "    is_NaN = df_history.isnull()\n",
        "    row_has_NaN = is_NaN.any(axis=1)\n",
        "    rows_with_NaN = df_history[row_has_NaN]\n",
        "    tickers = rows_with_NaN['Ticker'].unique()\n",
        "    df_history = df_history[~df_history['Ticker'].isin(tickers)]\n",
        "\n",
        "    df_history = df_history[df_history['Datetime'] <= last_month]\n",
        "    df_history = df_history.drop_duplicates().replace(np.inf, 0).replace(-np.inf,0).replace(0,0.001)\n",
        "\n",
        "    for fund in df_history[\"Ticker\"].unique():\n",
        "        if(len(df_history[df_history[\"Ticker\"] == fund]) < 12):\n",
        "            df_history = df_history[df_history[\"Ticker\"] != fund]\n",
        "\n",
        "    a = df_history[df_history['Datetime'] == last_month].Ticker.values\n",
        "    b = df_history.Ticker.unique()\n",
        "    intersection = list(set(a) & set(b))\n",
        "    fundos_faltantes = list(set(a) ^ set(b))\n",
        "\n",
        "    df_history = df_history[~df_history['Ticker'].isin(fundos_faltantes)]\n",
        "    \n",
        "    return df_history, df_adress\n",
        "\n",
        "def ajusta_desdobramento(df):\n",
        "    \n",
        "    # Desdobramentos obtidos em: https://br.investing.com/stock-split-calendar/\n",
        "    desdobramentos = {\n",
        "        \"BTCI11\": [\"2023-01\", 9],\n",
        "        \"CYCR11\": [\"2022-10\", 10],\n",
        "        \"EQIR11\": [\"2022-09\", 10],\n",
        "        \"VGIR11\": [\"2022-09\", 10],\n",
        "        \"GALG11\": [\"2022-08\", 10],\n",
        "        \"ARRI11\": [\"2022-08\", 10],\n",
        "        \"VIUR11\": [\"2022-05\", 10],\n",
        "        \"XPSF11\": [\"2022-05\", 10],\n",
        "        \"VIFI11\": [\"2022-04\", 10],\n",
        "        \"GAME11\": [\"2022-03\", 10],\n",
        "        \"BLMR11\": [\"2021-09\", 10],\n",
        "        \"MAXR11\": [\"2021-04\", 19],\n",
        "        \"RMAI11\": [\"2021-03\", 10],\n",
        "        \"FISC11\": [\"2020-12\", 10],\n",
        "        \"PQAG11\": [\"2020-11\", 10]\n",
        "    }\n",
        "\n",
        "    for key in desdobramentos:\n",
        "        if len(df[df[\"Ticker\"] == key]) > 0:\n",
        "            for index, row in df.iterrows():\n",
        "                if row[\"Ticker\"] == key and row[\"Datetime\"] < desdobramentos[key][0]:\n",
        "                    df.at[index,'Close'] = round(row['Close']/ desdobramentos[key][1],2)\n",
        "\n",
        "def getSectorMeans(df_funds, df_history):\n",
        "\n",
        "    df_setores = pd.DataFrame(({\n",
        "        'Setor':[],\n",
        "        'Datetime':[],\n",
        "        'DividendsChangeMean' :[],\n",
        "        'CloseChangeMean':[],\n",
        "        'DividendYieldChangeMean':[]\n",
        "    }))\n",
        "\n",
        "    for setor in df_funds[\"Setor\"].unique():\n",
        "\n",
        "        setor_tickers = df_funds[df_funds[\"Setor\"] == setor][\"Ticker\"].values\n",
        "\n",
        "        meme = df_history[df_history[\"Ticker\"].isin(setor_tickers)]\n",
        "        min_date = pd.to_datetime(meme[\"Datetime\"].min()).replace(day=1)\n",
        "        max_date = pd.to_datetime(meme[\"Datetime\"].max()).replace(day=1)\n",
        "\n",
        "        while min_date <= max_date:\n",
        "\n",
        "            date = (min_date).strftime(\"%Y-%m\")\n",
        "\n",
        "            df_setores = df_setores.append({\n",
        "                'Setor': setor, \n",
        "                'Datetime':date, \n",
        "                'DividendsChangeMean': meme[meme[\"Datetime\"] == date][\"DividendsChange\"].mean(), \n",
        "                'CloseChangeMean': meme[meme[\"Datetime\"] == date][\"CloseChange\"].mean(), \n",
        "                'DividendYieldChangeMean': meme[meme[\"Datetime\"] == date][\"DividendYieldChange\"].mean()\n",
        "            }, ignore_index=True)\n",
        "\n",
        "            min_date = min_date + relativedelta(months=1)\n",
        "    \n",
        "    return df_setores\n",
        "\n",
        "def get_ifix(years):\n",
        "\n",
        "    df_ifix = pd.DataFrame()\n",
        "    final_date = pd.to_datetime('today').strftime(\"%d-%m-%Y\").replace(\"-\",\"%2F\")\n",
        "    initial_date = str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) + \"-01-01\"\n",
        "\n",
        "    try:\n",
        "        df_ifix = pd.read_sql('df_ifix_' + str(initial_date) + \"_\" + str(final_date), engine)\n",
        "    except:\n",
        "\n",
        "        headers_aux = {\n",
        "            'authority':'www.infomoney.com.br',\n",
        "            'accept':'application/json, text/javascript, */*; q=0.01',\n",
        "            'accept-language':'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "            'content-type':'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "            'authority': 'www.infomoney.com.br',\n",
        "            'origin':'https://www.infomoney.com.br',\n",
        "            'referer':'https://www.infomoney.com.br/cotacoes/b3/indice/ifix/historico/',\n",
        "        }\n",
        "\n",
        "        body_aux = 'page=0&numberItems=99999&initialDate='+initial_date+'&finalDate='+final_date+'&symbol=IFIX'\n",
        "\n",
        "        response = requests.post('https://www.infomoney.com.br/wp-json/infomoney/v1/quotes/history', headers=headers_aux,  data=body_aux)\n",
        "\n",
        "        if not str(response.content) == \"b''\":\n",
        "\n",
        "            json_response = json.loads(response.content)\n",
        "\n",
        "            jobject = []\n",
        "            for obj in json_response:\n",
        "                jobject.append({\n",
        "                    'data': obj[0]['display'],\n",
        "                    'Close': obj[2]\n",
        "                })\n",
        "\n",
        "            df_ifix = pd.DataFrame(jobject)\n",
        "            df_ifix['Datetime'] = pd.to_datetime(df_ifix['data'], format='%d/%m/%Y')\n",
        "            df_ifix.drop(columns={'data'}, inplace = True)\n",
        "\n",
        "            if (len(df_ifix) > 1):\n",
        "                df_ifix.to_sql('df_ifix_' + str(initial_date) + \"_\" + str(final_date), engine, if_exists='replace', index=False)\n",
        "\n",
        "    return df_ifix\n",
        "\n",
        "def improveHistory(df_history, df_funds):\n",
        "\n",
        "    # Cria um array de índices\n",
        "    indices = ['Selic','IPCA','IGPM']\n",
        "\n",
        "    # Obtém o histórico do IFIX\n",
        "    df_ifix = get_ifix(2)\n",
        "    \n",
        "    # Obtém o histórico de índices\n",
        "    df_indices = {}\n",
        "    for indice in indices:\n",
        "        df_indices[indice] = obtem_dados_mercado(indice)\n",
        "\n",
        "    # Cria o DataFrame a ser aprimorado\n",
        "    df_improved = df_history.copy()\n",
        "\n",
        "    # Remove fundos que não possuem dados do mês anterior\n",
        "    df_improved = df_improved[df_improved['Datetime'] <= last_month]\n",
        "    df_improved = df_improved.drop_duplicates().replace(np.inf, 0).replace(-np.inf,0).replace(0,0.001)\n",
        "\n",
        "    # Remove fundos que não possuem pelo menos 12 registros\n",
        "    for fund in df_improved[\"Ticker\"].unique():\n",
        "        if(len(df_improved[df_improved[\"Ticker\"] == fund]) < 12):\n",
        "            df_improved = df_improved[df_improved[\"Ticker\"] != fund]\n",
        "\n",
        "    # Normaliza os dados que sofreram desdobramento\n",
        "    ajusta_desdobramento(df_improved)\n",
        "    df_improved = df_improved.replace(np.inf, 0).replace(-np.inf,0).replace(0,0.001)\n",
        "\n",
        "    # Cria a coluna DividendYield\n",
        "    df_improved['DividendYield'] = round(100*df_improved['Dividends']/df_improved['Close'],6)\n",
        "\n",
        "    # Cria novas colunas contendo a variação de valores ao longo dos meses\n",
        "    for index, fundo in enumerate(df_improved['Ticker'].unique()):\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendsChange'] = df_improved[df_improved.Ticker == fundo]['Dividends'].pct_change()\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'CloseChange'] = df_improved[df_improved.Ticker == fundo]['Close'].pct_change()\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendYieldChange'] = df_improved[df_improved.Ticker == fundo]['DividendYield'].pct_change()\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendsChange6M'] = df_improved[df_improved.Ticker == fundo]['Dividends'].pct_change(periods=6)\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'CloseChange6M'] = df_improved[df_improved.Ticker == fundo]['Close'].pct_change(periods=6)\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendYieldChange6M'] = df_improved[df_improved.Ticker == fundo]['DividendYield'].pct_change(periods=6)\n",
        "\n",
        "    # Remove linhas que possuam valor NaN\n",
        "    df_improved = df_improved.dropna(subset=['DividendsChange', 'CloseChange', 'DividendYieldChange', 'Close']).reset_index(drop=True)\n",
        "\n",
        "    # Procura no DataFrame registros que possuam uma variação de preço superior a 35%\n",
        "    drop_indexes = []\n",
        "    for index, fundo in enumerate(df_improved['Ticker'].unique()):\n",
        "        df_variacoes = df_improved[(abs(df_improved['CloseChange']) >= 0.35) & (df_improved[\"Ticker\"] == fundo)]\n",
        "        if len(df_variacoes) > 0:\n",
        "            drop_indexes = drop_indexes + list(df_improved[(df_improved[\"Datetime\"] <= df_variacoes[\"Datetime\"].values[-1]) & (df_improved.Ticker == fundo)].index)\n",
        "    \n",
        "    # Remove todos os registros de datas anteriores às variações de 35%\n",
        "    df_improved = df_improved.drop(drop_indexes)\n",
        "    df_sectors = getSectorMeans(df_funds, df_improved)\n",
        "\n",
        "    # Cria as colunas dos índices\n",
        "    for indice in indices:\n",
        "        df_improved[indice] = float(\"NaN\")\n",
        "                    \n",
        "    # Insere preço dos índices e a média do setor ao longo do tempo\n",
        "    for index, fundo in enumerate(df_improved['Ticker'].unique()):\n",
        "\n",
        "        print(str(index+1) + \"/\" + str(len(df_improved['Ticker'].unique())))\n",
        "\n",
        "        sector = df_funds[df_funds[\"Ticker\"] == fundo][\"Setor\"].values[0]\n",
        "\n",
        "        for data in df_improved['Datetime']:\n",
        "\n",
        "            df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), \"IFIX\"] = get_month_close(df_ifix, data)\n",
        "\n",
        "            sector_values = df_sectors[(df_sectors[\"Datetime\"] == data) & (df_sectors[\"Setor\"] == sector)]\n",
        "\n",
        "            if len(sector_values) > 0:\n",
        "                for mean_col in [\"DividendsChangeMean\", \"CloseChangeMean\", \"DividendYieldChangeMean\"]:\n",
        "                    mean_value = sector_values[mean_col].values[0]\n",
        "                    df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), \"Sector\" + mean_col] = float(mean_value)\n",
        "\n",
        "            for indice in indices:\n",
        "                indice_values = df_indices[indice][df_indices[indice].Timestamp == data]['Value'].values\n",
        "                if len(indice_values) > 0:\n",
        "                    df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), indice] = float(indice_values[0])\n",
        "\n",
        "\n",
        "\n",
        "    return df_improved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY3_H3-ev1Iu"
      },
      "outputs": [],
      "source": [
        "df_funds = get_all_funds()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_history, df_adress = process_history(df_funds, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzFenxM4xVCE",
        "outputId": "636defe4-fcdf-490b-a860-1a5516f71447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coletando informações de ABCP11...\n",
            "FII ABCP11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de AFHI11...\n",
            "22 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de AIEC11...\n",
            "26 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de ALMI11...\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de ALZM11...\n",
            "20 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de ALZR11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 12 endereços foram encontrados.\n",
            "Coletando informações de APTO11...\n",
            "15 dados de histórico e 5 endereços foram encontrados.\n",
            "Coletando informações de ARCT11...\n",
            "25 dados de histórico e 7 endereços foram encontrados.\n",
            "Coletando informações de ARRI11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de ATSA11...\n",
            "6 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de BARI11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BBFI11B...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de BBFO11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BBGO11...\n",
            "14 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BBIM11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BBPO11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 64 endereços foram encontrados.\n",
            "Coletando informações de BBRC11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "26 dados de histórico e 20 endereços foram encontrados.\n",
            "Coletando informações de BCFF11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BCIA11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BCRI11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BICE11...\n",
            "FII BICE11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BICR11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BLCA11...\n",
            "16 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de BLCP11...\n",
            "21 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de BLMC11...\n",
            "24 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BLMG11...\n",
            "25 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de BLMO11...\n",
            "13 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de BLMR11...\n",
            "24 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BMLC11...\n",
            "25 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de BNFS11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 18 endereços foram encontrados.\n",
            "Coletando informações de BPFF11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BPML11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "FII BPML11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 3 endereços foram encontrados.\n",
            "Coletando informações de BRCO11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 10 endereços foram encontrados.\n",
            "Coletando informações de BRCR11...\n",
            "FII BRCR11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 4 endereços foram encontrados.\n",
            "Coletando informações de BREV11...\n",
            "25 dados de histórico e 14 endereços foram encontrados.\n",
            "Coletando informações de BRIP11...\n",
            "1 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BRLA11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 3 endereços foram encontrados.\n",
            "Coletando informações de BTAL11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "24 dados de histórico e 9 endereços foram encontrados.\n",
            "Coletando informações de BTCR11...\n",
            "22 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BTLG11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 9 endereços foram encontrados.\n",
            "Coletando informações de BTRA11...\n",
            "21 dados de histórico e 5 endereços foram encontrados.\n",
            "Coletando informações de BTSG11...\n",
            "14 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de BTSI11...\n",
            "3 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de BTWR11...\n",
            "20 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de BZLI11...\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de CACR11...\n",
            "28 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de CARE11...\n",
            "8 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de CBOP11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de CCRF11...\n",
            "21 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de CEOC11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de CJCT11...\n",
            "1 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de CNES11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de CORM11...\n",
            "16 dados de histórico e 3 endereços foram encontrados.\n",
            "Coletando informações de CPFF11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de CPTS11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de CRFF11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de CTXT11...\n",
            "7 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de CVBI11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de CXAG11...\n",
            "15 dados de histórico e 32 endereços foram encontrados.\n",
            "Coletando informações de CXCE11B...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "26 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de CXCO11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "22 dados de histórico e 10 endereços foram encontrados.\n",
            "Coletando informações de CXRI11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de CXTL11...\n",
            "FII CXTL11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 3 endereços foram encontrados.\n",
            "Coletando informações de CYCR11...\n",
            "17 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de DEVA11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de DRIT11B...\n",
            "25 dados de histórico e 6 endereços foram encontrados.\n",
            "Coletando informações de DVFF11...\n",
            "21 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de EDFO11B...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de EDGA11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de EQIR11...\n",
            "15 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de ERCR11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de EURO11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "26 dados de histórico e 3 endereços foram encontrados.\n",
            "Coletando informações de EVBI11...\n",
            "FII EVBI11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 4 endereços foram encontrados.\n",
            "Coletando informações de FAED11...\n",
            "25 dados de histórico e 3 endereços foram encontrados.\n",
            "Coletando informações de FAMB11B...\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de FATN11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 19 endereços foram encontrados.\n",
            "Coletando informações de FCFL11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de FEXC11...\n",
            "24 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de FIGS11...\n",
            "25 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de FIIB11...\n",
            "FII FIIB11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de FIIP11B...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 7 endereços foram encontrados.\n",
            "Coletando informações de FISC11...\n",
            "FII FISC11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de FIVN11...\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de FLCR11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de FLMA11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de FLRP11...\n",
            "FII FLRP11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de FMOF11...\n",
            "FII FMOF11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de FPAB11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de FVPQ11...\n",
            "FII FVPQ11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de GALG11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "27 dados de histórico e 6 endereços foram encontrados.\n",
            "Coletando informações de GAME11...\n",
            "18 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de GCFF11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de GCRA11...\n",
            "17 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de GCRI11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de GESE11B...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de GGRC11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 14 endereços foram encontrados.\n",
            "Coletando informações de GSFI11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "0 dados de histórico e 9 endereços foram encontrados.\n",
            "Coletando informações de GTWR11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 3 endereços foram encontrados.\n",
            "Coletando informações de HAAA11...\n",
            "25 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de HABT11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de HBRH11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "24 dados de histórico e 5 endereços foram encontrados.\n",
            "Coletando informações de HCHG11...\n",
            "FII HCHG11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de HCRI11...\n",
            "FII HCRI11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de HCTR11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de HFOF11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de HGBS11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 12 endereços foram encontrados.\n",
            "Coletando informações de HGCR11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de HGFF11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de HGIC11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de HGLG11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 16 endereços foram encontrados.\n",
            "Coletando informações de HGPO11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de HGRE11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 16 endereços foram encontrados.\n",
            "Coletando informações de HGRS11...\n",
            "3 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de HGRU11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "26 dados de histórico e 77 endereços foram encontrados.\n",
            "Coletando informações de HLOG11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 3 endereços foram encontrados.\n",
            "Coletando informações de HOSI11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de HPDP11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de HRDF11...\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de HREC11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de HSAF11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de HSLG11...\n",
            "25 dados de histórico e 4 endereços foram encontrados.\n",
            "Coletando informações de HSML11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 6 endereços foram encontrados.\n",
            "Coletando informações de HSRE11...\n",
            "25 dados de histórico e 26 endereços foram encontrados.\n",
            "Coletando informações de HTMX11...\n",
            "6 dados de histórico e 21 endereços foram encontrados.\n",
            "Coletando informações de HUSC11...\n",
            "26 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de HUSI11...\n",
            "7 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de IBCR11...\n",
            "20 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de IBFF11...\n",
            "21 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de IDFI11...\n",
            "17 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de IRDM11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de IRIM11...\n",
            "17 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de ITIP11...\n",
            "23 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de ITIT11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de JFLL11...\n",
            "24 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de JGPX11...\n",
            "14 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de JPPA11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de JRDM11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "FII JRDM11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de JSAF11...\n",
            "19 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de JSRE11...\n",
            "25 dados de histórico e 6 endereços foram encontrados.\n",
            "Coletando informações de KCRE11...\n",
            "8 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de KEVE11...\n",
            "FII KEVE11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de KFOF11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de KINP11...\n",
            "FII KINP11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de KISU11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de KNCA11...\n",
            "15 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de KNCR11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de KNHY11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de KNIP11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de KNRE11...\n",
            "FII KNRE11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de KNRI11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 20 endereços foram encontrados.\n",
            "Coletando informações de KNSC11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de LASC11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "26 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de LFTT11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "FII LFTT11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de LGCP11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 5 endereços foram encontrados.\n",
            "Coletando informações de LUGG11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 4 endereços foram encontrados.\n",
            "Coletando informações de LVBI11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "FII LVBI11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 7 endereços foram encontrados.\n",
            "Coletando informações de MALL11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 8 endereços foram encontrados.\n",
            "Coletando informações de MATV11...\n",
            "18 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de MAXR11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "FII MAXR11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 5 endereços foram encontrados.\n",
            "Coletando informações de MBRF11...\n",
            "23 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de MCCI11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de MCHF11...\n",
            "20 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de MCHY11...\n",
            "20 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de MFAI11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de MFII11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de MGCR11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de MGFF11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de MGHT11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de MORC11...\n",
            "21 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de MORE11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de MXRF11...\n",
            "25 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de NAVT11...\n",
            "22 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de NCHB11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de NCRI11...\n",
            "2 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de NEWL11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "27 dados de histórico e 3 endereços foram encontrados.\n",
            "Coletando informações de NEWU11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "3 dados de histórico e 3 endereços foram encontrados.\n",
            "Coletando informações de NSLU11...\n",
            "FII NSLU11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de NVHO11...\n",
            "26 dados de histórico e 4 endereços foram encontrados.\n",
            "Coletando informações de NVIF11B...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "FII NVIF11B será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de ONEF11...\n",
            "26 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de ORPD11...\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de OUFF11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de OUJP11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de OULG11...\n",
            "FII OULG11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de OURE11...\n",
            "25 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de PABY11...\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de PATC11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "28 dados de histórico e 6 endereços foram encontrados.\n",
            "Coletando informações de PATL11...\n",
            "25 dados de histórico e 4 endereços foram encontrados.\n",
            "Coletando informações de PLCR11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de PLOG11...\n",
            "23 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de PLRI11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de PORD11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de PQAG11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de PQDP11...\n",
            "FII PQDP11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de PRSV11...\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de PVBI11...\n",
            "FII PVBI11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 3 endereços foram encontrados.\n",
            "Coletando informações de QAGR11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "FII QAGR11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 12 endereços foram encontrados.\n",
            "Coletando informações de QAMI11...\n",
            "22 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de QIRI11...\n",
            "21 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RBCO11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "22 dados de histórico e 4 endereços foram encontrados.\n",
            "Coletando informações de RBDS11...\n",
            "FII RBDS11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RBED11...\n",
            "25 dados de histórico e 7 endereços foram encontrados.\n",
            "Coletando informações de RBFF11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RBGS11...\n",
            "12 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de RBHG11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RBHY11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RBIR11...\n",
            "FII RBIR11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RBLG11...\n",
            "FII RBLG11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de RBRD11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 4 endereços foram encontrados.\n",
            "Coletando informações de RBRF11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RBRL11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 5 endereços foram encontrados.\n",
            "Coletando informações de RBRP11...\n",
            "25 dados de histórico e 10 endereços foram encontrados.\n",
            "Coletando informações de RBRR11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RBRS11...\n",
            "26 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de RBRY11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RBTS11...\n",
            "FII RBTS11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RBVA11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 78 endereços foram encontrados.\n",
            "Coletando informações de RBVO11...\n",
            "FII RBVO11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RCRB11...\n",
            "26 dados de histórico e 8 endereços foram encontrados.\n",
            "Coletando informações de RDPD11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RECR11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RECT11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 9 endereços foram encontrados.\n",
            "Coletando informações de RECX11...\n",
            "22 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RELG11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 5 endereços foram encontrados.\n",
            "Coletando informações de RFOF11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RMAI11...\n",
            "17 dados de histórico e 2 endereços foram encontrados.\n",
            "Coletando informações de RNDP11...\n",
            "27 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RNGO11...\n",
            "25 dados de histórico e 4 endereços foram encontrados.\n",
            "Coletando informações de RRCI11...\n",
            "27 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RSPD11...\n",
            "FII RSPD11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RVBI11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RZAG11...\n",
            "15 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RZAK11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de RZTR11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "28 dados de histórico e 15 endereços foram encontrados.\n",
            "Coletando informações de SADI11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de SARE11...\n",
            "25 dados de histórico e 3 endereços foram encontrados.\n",
            "Coletando informações de SCPF11...\n",
            "FII SCPF11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de SDIL11...\n",
            "25 dados de histórico e 5 endereços foram encontrados.\n",
            "Coletando informações de SEQR11...\n",
            "24 dados de histórico e 4 endereços foram encontrados.\n",
            "Coletando informações de SHPH11...\n",
            "FII SHPH11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de SNCI11...\n",
            "17 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de SNFF11...\n",
            "21 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de SNLG11...\n",
            "22 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de SPTW11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de SPVJ11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "23 dados de histórico e 9 endereços foram encontrados.\n",
            "Coletando informações de SRVD11...\n",
            "3 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de STRX11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de TEPP11...\n",
            "25 dados de histórico e 5 endereços foram encontrados.\n",
            "Coletando informações de TGAR11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de TORD11...\n",
            "FII TORD11 será removido por estar com dados faltantes.\n",
            "0 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de TRNT11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de TRXF11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 24 endereços foram encontrados.\n",
            "Coletando informações de URPR11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de VCJR11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de VCRA11...\n",
            "8 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de VCRR11...\n",
            "20 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de VGHF11...\n",
            "23 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de VGIA11...\n",
            "14 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de VGIP11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de VGIR11...\n",
            "25 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de VIFI11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de VILG11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 15 endereços foram encontrados.\n",
            "Coletando informações de VINO11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 10 endereços foram encontrados.\n",
            "Coletando informações de VISC11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "25 dados de histórico e 19 endereços foram encontrados.\n",
            "Coletando informações de VIUR11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "20 dados de histórico e 5 endereços foram encontrados.\n",
            "Coletando informações de VJFD11...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\n",
            "19 dados de histórico e 31 endereços foram encontrados.\n",
            "Coletando informações de VLOL11...\n",
            "25 dados de histórico e 1 endereços foram encontrados.\n",
            "Coletando informações de VOTS11...\n",
            "26 dados de histórico e 0 endereços foram encontrados.\n",
            "Coletando informações de VRTA11...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "\n",
        "# conexão com o banco de dados\n",
        "#conn = psycopg2.connect(\n",
        "#    host=\"kesavan.db.elephantsql.com\",\n",
        "#    database=\"wzmywfei\",\n",
        "#    user=\"wzmywfei\",\n",
        "#    password=\"yU9UYTEgfnTRQVkBF_oBcSCwLJtzmd5r\"\n",
        "#)\n",
        "# engine = create_engine('postgresql://wzmywfei:yU9UYTEgfnTRQVkBF_oBcSCwLJtzmd5r@kesavan.db.elephantsql.com/wzmywfei', echo=False)\n",
        "\n",
        "conn = psycopg2.connect(\n",
        "    host=\"babar.db.elephantsql.com\",\n",
        "    database=\"qgfnmxab\",\n",
        "    user=\"qgfnmxab\",\n",
        "    password=\"e0-o4nW4SHQ7sazzIvpKGv7Vc_OFV5Yi\"\n",
        ")\n",
        "\n",
        "# cria um cursor para executar consultas\n",
        "cur = conn.cursor()\n",
        "\n",
        "# concede todas as permissões ao usuário\n",
        "cur.execute(\"GRANT ALL PRIVILEGES ON DATABASE qgfnmxab TO qgfnmxab\")\n",
        "\n",
        "cur.execute(\"CREATE TABLE df_history (Ticker TEXT, Datetime TEXT, Dividends FLOAT(53), Close FLOAT(53))\")\n",
        "\n",
        "# fecha a conexão e o cursor\n",
        "cur.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "mw6G7nMj6tV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "engine = create_engine('postgresql://qgfnmxab:e0-o4nW4SHQ7sazzIvpKGv7Vc_OFV5Yi@babar.db.elephantsql.com/qgfnmxab', echo=False)"
      ],
      "metadata": {
        "id": "HAVyitak7pCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyMxLHB2OpBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4488022-402d-4328-ffbd-d8dd17a8f59a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4599 históricos de fundos imobiliários foram encontrados.\n",
            "1091 endereços de fundos imobiliários foram encontrados.\n",
            "21.72% dos endereços estão sem Latitude e Longitude.\n"
          ]
        }
      ],
      "source": [
        "# Obtém o histórico de todos os fundos imobiliários existentes\n",
        "try:\n",
        "    df_adress = pd.read_sql('df_adress_' + last_month, engine) \n",
        "    df_history = pd.read_sql('df_history_' + last_month, engine) \n",
        "except:\n",
        "    df_history, df_adress = process_history(df_funds, 2)\n",
        "    df_adress.to_sql('df_adress_' + last_month, engine, if_exists='replace', index=False)\n",
        "    df_history.to_sql('df_history_' + last_month, engine, if_exists='replace', index=False)\n",
        "\n",
        "print(str(len(df_history)) + \" históricos de fundos imobiliários foram encontrados.\")\n",
        "print(str(len(df_adress)) + \" endereços de fundos imobiliários foram encontrados.\")\n",
        "\n",
        "percent = df_adress['Latitude'].isnull().sum()/(len(df_adress))*100\n",
        "print(\"%.2f%% dos endereços estão sem Latitude e Longitude.\" % percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RKEgq5dDSO4"
      },
      "outputs": [],
      "source": [
        "# Obtém o histórico de todos os fundos imobiliários existentes diários\n",
        "# try:\n",
        "#     df_history_daily = pd.read_sql('df_history_daily_' + last_month, engine)\n",
        "# except:\n",
        "#     df_history_daily = process_daily_history(df_history, 2)\n",
        "#     df_history_daily.to_sql('df_history_daily_' + last_month, engine, if_exists='replace', index=False)\n",
        "\n",
        "# Obtém o histórico de todos os fundos imobiliários existentes diários\n",
        "# try:\n",
        "#     df_history_daily = pd.read_sql('df_history_daily_completo_' + last_month, engine)\n",
        "# except:\n",
        "#     df_history_daily = preenche_historico_faltante(df_history_daily)\n",
        "#     df_history_daily.to_sql('df_history_daily_completo_' + last_month, engine, if_exists='replace', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foMwaAnCtsBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af67ff5-6855-402a-e25e-2d3b7f2fce36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/199\n",
            "2/199\n",
            "3/199\n",
            "4/199\n",
            "5/199\n",
            "6/199\n",
            "7/199\n",
            "8/199\n",
            "9/199\n",
            "10/199\n",
            "11/199\n",
            "12/199\n",
            "13/199\n",
            "14/199\n",
            "15/199\n",
            "16/199\n",
            "17/199\n",
            "18/199\n",
            "19/199\n",
            "20/199\n",
            "21/199\n",
            "22/199\n",
            "23/199\n",
            "24/199\n",
            "25/199\n",
            "26/199\n",
            "27/199\n",
            "28/199\n",
            "29/199\n",
            "30/199\n",
            "31/199\n",
            "32/199\n",
            "33/199\n",
            "34/199\n",
            "35/199\n",
            "36/199\n",
            "37/199\n",
            "38/199\n",
            "39/199\n",
            "40/199\n",
            "41/199\n",
            "42/199\n",
            "43/199\n",
            "44/199\n",
            "45/199\n",
            "46/199\n",
            "47/199\n",
            "48/199\n",
            "49/199\n",
            "50/199\n",
            "51/199\n",
            "52/199\n",
            "53/199\n",
            "54/199\n",
            "55/199\n",
            "56/199\n",
            "57/199\n",
            "58/199\n",
            "59/199\n",
            "60/199\n",
            "61/199\n",
            "62/199\n",
            "63/199\n",
            "64/199\n",
            "65/199\n",
            "66/199\n",
            "67/199\n",
            "68/199\n",
            "69/199\n",
            "70/199\n",
            "71/199\n",
            "72/199\n",
            "73/199\n",
            "74/199\n",
            "75/199\n",
            "76/199\n",
            "77/199\n",
            "78/199\n",
            "79/199\n",
            "80/199\n",
            "81/199\n",
            "82/199\n",
            "83/199\n",
            "84/199\n",
            "85/199\n",
            "86/199\n",
            "87/199\n",
            "88/199\n",
            "89/199\n",
            "90/199\n",
            "91/199\n",
            "92/199\n",
            "93/199\n",
            "94/199\n",
            "95/199\n",
            "96/199\n",
            "97/199\n",
            "98/199\n",
            "99/199\n",
            "100/199\n",
            "101/199\n",
            "102/199\n",
            "103/199\n",
            "104/199\n",
            "105/199\n",
            "106/199\n",
            "107/199\n",
            "108/199\n",
            "109/199\n",
            "110/199\n",
            "111/199\n",
            "112/199\n",
            "113/199\n",
            "114/199\n",
            "115/199\n",
            "116/199\n",
            "117/199\n",
            "118/199\n",
            "119/199\n",
            "120/199\n",
            "121/199\n",
            "122/199\n",
            "123/199\n",
            "124/199\n",
            "125/199\n",
            "126/199\n",
            "127/199\n",
            "128/199\n",
            "129/199\n",
            "130/199\n",
            "131/199\n",
            "132/199\n",
            "133/199\n",
            "134/199\n",
            "135/199\n",
            "136/199\n",
            "137/199\n",
            "138/199\n",
            "139/199\n",
            "140/199\n",
            "141/199\n",
            "142/199\n",
            "143/199\n",
            "144/199\n",
            "145/199\n",
            "146/199\n",
            "147/199\n",
            "148/199\n",
            "149/199\n",
            "150/199\n",
            "151/199\n",
            "152/199\n",
            "153/199\n",
            "154/199\n",
            "155/199\n",
            "156/199\n",
            "157/199\n",
            "158/199\n",
            "159/199\n",
            "160/199\n",
            "161/199\n",
            "162/199\n",
            "163/199\n",
            "164/199\n",
            "165/199\n",
            "166/199\n",
            "167/199\n",
            "168/199\n",
            "169/199\n",
            "170/199\n",
            "171/199\n",
            "172/199\n",
            "173/199\n",
            "174/199\n",
            "175/199\n",
            "176/199\n",
            "177/199\n",
            "178/199\n",
            "179/199\n",
            "180/199\n",
            "181/199\n",
            "182/199\n",
            "183/199\n",
            "184/199\n",
            "185/199\n"
          ]
        }
      ],
      "source": [
        "# Obtém o histórico aprimorado de todos os fundos imobiliários existentes\n",
        "try:\n",
        "    df_history = pd.read_sql('df_history_improved_' + last_month, engine)\n",
        "except:\n",
        "    df_history = improveHistory(df_history, df_funds)\n",
        "    df_history.to_sql('df_history_improved_' + last_month, engine, if_exists='replace', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTmkOSD8A92K",
        "outputId": "8d30ebfb-4388-45cb-d2d8-b5216a920505"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "209"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(df_history.Ticker.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fcfh9R12GuD",
        "outputId": "1c6c945f-72ac-4508-857c-a2d4cd47a288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "194 FIIs restaram\n"
          ]
        }
      ],
      "source": [
        "print(str(len(df_history[\"Ticker\"].unique())) + \" FIIs existiam inicialmente\")\n",
        "\n",
        "#df_history = df_history[df_history['Datetime'] <= last_month]\n",
        "#df_history = df_history.drop_duplicates().replace(np.inf, 0).replace(-np.inf,0).replace(0,0.001)\n",
        "\n",
        "#for fund in df_history[\"Ticker\"].unique():\n",
        "#    if(len(df_history[df_history[\"Ticker\"] == fund]) < 12):\n",
        "#        df_history = df_history[df_history[\"Ticker\"] != fund]\n",
        "\n",
        "a = df_history[df_history['Datetime'] == last_month].Ticker.values\n",
        "b = df_history.Ticker.unique()\n",
        "intersection = list(set(a) & set(b))\n",
        "fundos_faltantes = list(set(a) ^ set(b))\n",
        "\n",
        "df_history = df_history[~df_history['Ticker'].isin(fundos_faltantes)]\n",
        "\n",
        "print(str(len(df_history[\"Ticker\"].unique())) + \" FIIs restaram\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHjJfYeVwdDX"
      },
      "source": [
        "# 2. Data Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiUpK1uAufh3"
      },
      "outputs": [],
      "source": [
        "def prepare_errors_array(possibilities, strategy):\n",
        "\n",
        "    errors = {'CloseChange_Arima': []}\n",
        "\n",
        "    possibilities = list(filter(lambda x: len(x) > 3, possibilities))\n",
        "\n",
        "    for possibility in possibilities:\n",
        "        errors['CloseChange_'+strategy+'_' + '_'.join(possibility)] = []\n",
        "\n",
        "    return errors, possibilities\n",
        "\n",
        "def upload_rmse(pred_col, train_cols, strategy, sectors, rmse_array, params = None):\n",
        "\n",
        "    with mlflow.start_run(run_name=train_cols):   \n",
        "        \n",
        "        df_rmse = pd.DataFrame(rmse_array, columns = ['rmse'])\n",
        "\n",
        "        # Parameters\n",
        "        mlflow.log_param(\"pred_col\", pred_col)\n",
        "        mlflow.log_param(\"train_cols\", train_cols)\n",
        "        mlflow.log_param(\"strategy\", strategy)\n",
        "        mlflow.log_param(\"sector\", ', '.join(sectors))\n",
        "\n",
        "        # Statistical Metrics\n",
        "        mlflow.log_metric(\"count\", df_rmse.describe().values[0][0])\n",
        "        mlflow.log_metric(\"mean\", df_rmse.describe().values[1][0])\n",
        "        mlflow.log_metric(\"std\", df_rmse.describe().values[2][0])\n",
        "        mlflow.log_metric(\"min\", df_rmse.describe().values[3][0])\n",
        "        mlflow.log_metric(\"25 pct.\", df_rmse.describe().values[4][0])\n",
        "        mlflow.log_metric(\"50 pct.\", df_rmse.describe().values[5][0])\n",
        "        mlflow.log_metric(\"75 pct.\", df_rmse.describe().values[6][0])\n",
        "        mlflow.log_metric(\"max\", df_rmse.describe().values[7][0])\n",
        "\n",
        "        # Machine Learning Params\n",
        "        mlflow.log_metric(\"n_estimators\", 0.0 if params == None else params[\"n_estimators\"]) \n",
        "        mlflow.log_metric(\"learning_rate\", 0.0 if params == None else params[\"learning_rate\"]) \n",
        "        mlflow.log_metric(\"max_depth\", 0.0 if params == None else params[\"max_depth\"]) \n",
        "        mlflow.log_metric(\"min_child_weight\", 0.0 if params == None else params[\"min_child_weight\"]) \n",
        "        mlflow.log_metric(\"colsample_bytree\", 0.0 if params == None else params[\"colsample_bytree\"])\n",
        "\n",
        "def get_possibilities(target_column, training_columns):\n",
        "\n",
        "    possibilities = []\n",
        "    for L in range(len(training_columns) + 1):\n",
        "        for subset in itertools.combinations(training_columns, L):\n",
        "            possibilities.append([target_column] + list(subset))\n",
        "\n",
        "    possibilities.reverse()\n",
        "\n",
        "    return possibilities\n",
        "\n",
        "def filtra_tipo(df_history, tipo):\n",
        "\n",
        "    tickers = list(df_funds['Ticker'].values)\n",
        "\n",
        "    if tipo == \"Papel\":\n",
        "        tickers = list(df_funds[(df_funds[\"Setor\"] == \"Títulos e Val. Mob.\")]['Ticker'].values)\n",
        "    elif tipo == \"Hibrido\":\n",
        "        tickers = list(df_funds[(df_funds[\"Setor\"] == \"Híbrido\")]['Ticker'].values)\n",
        "    elif tipo == \"Tijolo\":\n",
        "        tickers = list(df_funds[(df_funds[\"Setor\"] != \"Títulos e Val. Mob.\") & (df_funds[\"Setor\"] != \"Híbrido\")]['Ticker'].values)\n",
        "    \n",
        "    return df_history[df_history['Ticker'].isin(tickers)]\n",
        "\n",
        "def train_test_split(data, perc):\n",
        "\n",
        "    data = data.values\n",
        "    n = int(len(data) * (1 - perc))\n",
        "    return data[:n], data[n:]\n",
        "\n",
        "def model_predict(train, test, val, model):\n",
        "\n",
        "    test = np.array([test])\n",
        "    train = np.array(train)\n",
        "    val = np.array(val)\n",
        "\n",
        "    X, y = train[:, :-1], train[:, -1]\n",
        "\n",
        "    if(len(val) > 0):\n",
        "        X_val, y_val = val[:, :-1], val[:, -1]\n",
        "        model.fit(X, y, eval_set=[(X,y),(X_val, y_val)], verbose=0)\n",
        "    else:\n",
        "        model.fit(X, y)\n",
        "    \n",
        "    pred = model.predict(test)\n",
        "\n",
        "    return pred[0]\n",
        "\n",
        "def arima_predict(df_history, fundo, pred_col, pred_index):\n",
        "\n",
        "    df = df_history[df_history['Ticker'] == fundo].copy()\n",
        "    df['Target'] = df[pred_col].shift(-pred_index)\n",
        "    df = df[[pred_col, 'Target']]\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    predictions = []\n",
        "    train, test = train_test_split(df, 0.4) # 60% de treino\n",
        "\n",
        "    # cria a variável history\n",
        "    history = [x[0] for x in train]\n",
        "\n",
        "    for i in range(len(test)):\n",
        "        test_X, test_y = test[i, :-1], test[i, -1]\n",
        "\n",
        "        # O arima deverá considerar apenas o valor da coluna principal\n",
        "\n",
        "        model = SARIMAX(history, order=(1,1,1))\n",
        "        resultado_sarimax = model.fit(maxiter=100)\n",
        "\n",
        "        # Obtém a predição de {pred_index} meses à frente\n",
        "        output = resultado_sarimax.get_forecast(steps=pred_index)\n",
        "\n",
        "        # Obtém a predição do mês de interesse\n",
        "        pred = output.predicted_mean[pred_index-1]\n",
        "\n",
        "        predictions.append(pred)\n",
        "        history.append(test[i][0])\n",
        "\n",
        "    # evaluate forecasts\n",
        "    yhat = round(predictions[0],2)\n",
        "    rmse = round(mean_squared_error(test[:, -1], predictions, squared=False),6)\n",
        "\n",
        "    return yhat, rmse\n",
        "\n",
        "def machinelearn_predict(df_history, fundo, pred_col, train_cols, params, pred_index):\n",
        " \n",
        "    df = df_history[df_history['Ticker'] == fundo][train_cols].copy()\n",
        "    df[\"Target\"] = df[pred_col].shift(-pred_index)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    model = None\n",
        "    validation = []\n",
        "    predictions = []\n",
        "    train, test = train_test_split(df, 0.4) # 60% de treino\n",
        "\n",
        "    if('early_stopping_rounds' in params):\n",
        "        validation = test[:len(test)//2] # 20% de validação\n",
        "        test = test[len(test)//2:] # 20% de teste\n",
        "\n",
        "    history = [x for x in train]\n",
        "\n",
        "    for i in range(len(test)):\n",
        "        test_X, test_y = test[i, :-1], test[i, -1]\n",
        "\n",
        "        model = XGBRegressor()\n",
        "        model.set_params(**params)\n",
        "\n",
        "        pred = model_predict(history, test_X, validation, model)\n",
        "\n",
        "        predictions.append(pred)\n",
        "\n",
        "        history.append(test[i])\n",
        "\n",
        "    # evaluate forecasts\n",
        "    yhat = round(predictions[0],2)\n",
        "    rmse = round(mean_squared_error(test[:, -1], predictions, squared=False),6)\n",
        "    #absolute_error = abs(test[:, -1] - predictions)\n",
        "    return yhat, rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH5n4OJl9mWI",
        "outputId": "5bc2e9d7-9fe2-489c-c515-0062f8603248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=4, read=5, redirect=5, status=5)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fcd7dac1f70>: Failed to establish a new connection: [Errno -2] Name or service not known')': /henrique.p.diniz/Projeto-Pesquisa-Mestrado.mlflow/api/2.0/mlflow/experiments/get-by-name?experiment_name=FII-Tijolo-1M-202301\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=3, connect=3, read=5, redirect=5, status=5)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fcd8134bd90>: Failed to establish a new connection: [Errno -2] Name or service not known')': /henrique.p.diniz/Projeto-Pesquisa-Mestrado.mlflow/api/2.0/mlflow/experiments/get-by-name?experiment_name=FII-Tijolo-1M-202301\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=2, read=5, redirect=5, status=5)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fcd8134b190>: Failed to establish a new connection: [Errno -2] Name or service not known')': /henrique.p.diniz/Projeto-Pesquisa-Mestrado.mlflow/api/2.0/mlflow/experiments/get-by-name?experiment_name=FII-Tijolo-1M-202301\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=1, read=5, redirect=5, status=5)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fcd8134b0a0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /henrique.p.diniz/Projeto-Pesquisa-Mestrado.mlflow/api/2.0/mlflow/experiments/get-by-name?experiment_name=FII-Tijolo-1M-202301\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=0, read=5, redirect=5, status=5)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fcd8132e4c0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /henrique.p.diniz/Projeto-Pesquisa-Mestrado.mlflow/api/2.0/mlflow/experiments/get-by-name?experiment_name=FII-Tijolo-1M-202301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nenhum experimento foi encontrado\n",
            "SETORES EXISTENTES:\n",
            " - Títulos e Val. Mob.\n",
            " - Lajes Corporativas\n",
            " - Logística\n",
            " - Híbrido\n",
            " - Outros\n",
            " - Shoppings\n",
            " - Residencial\n",
            " - Hotel\n",
            " - Hospital\n"
          ]
        }
      ],
      "source": [
        "sectors = pd.merge(df_funds, df_history, on='Ticker')[\"Setor\"].unique()\n",
        "existent_experiments = []\n",
        "\n",
        "try:\n",
        "  existent_experiments = list(get_experiments_result(experiment_name)[\"params.train_cols\"])\n",
        "except:\n",
        "  print(\"Nenhum experimento foi encontrado\")\n",
        "\n",
        "print(\"\\n - \".join([\"SETORES EXISTENTES:\"] + list(sectors)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUYyLBVDD8b6",
        "outputId": "8cdce109-aed0-4849-b0dc-99131681577d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86\n"
          ]
        }
      ],
      "source": [
        "df_history = filtra_tipo(df_history, tipo_interesse) \n",
        "print(len(df_history.Ticker.unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8y2iNSvdN_M"
      },
      "outputs": [],
      "source": [
        "train_arima = False\n",
        "\n",
        "if train_arima: \n",
        "    rmse_array = []\n",
        "    pred_column = 'CloseChange'\n",
        "\n",
        "    if 'Arima' not in existent_experiments:\n",
        "        for i, fundo in enumerate(df_history[\"Ticker\"].unique()):\n",
        "\n",
        "            print(\"Calculating errors of \"+ fundo + \": \" + str(i+1) + \"/\" + str(len(df_history[\"Ticker\"].unique())))\n",
        "\n",
        "            prediction, rmse = arima_predict(df_history, fundo, pred_column, janela_treino)\n",
        "            rmse_array.append(rmse)\n",
        "\n",
        "        upload_rmse(pred_column, pred_column, 'Arima', sectors, rmse_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHaiq-e3spev",
        "outputId": "914d91d6-9647-4e4d-bde2-6ed3e04f3fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating \"CloseChange, Close, DividendYield, DividendsChange, DividendYieldChange, SectorDividendsChangeMean, Selic, IPCA, IGPM, IFIX, SectorCloseChangeMean\": 1/968\n",
            "Calculating errors of AIEC11: 1/86\n",
            "Calculating errors of ALZR11: 2/86\n",
            "Calculating errors of BBFI11B: 3/86\n",
            "Calculating errors of BBPO11: 4/86\n",
            "Calculating errors of BBRC11: 5/86\n",
            "Calculating errors of BLMC11: 6/86\n",
            "Calculating errors of BLMG11: 7/86\n",
            "Calculating errors of BMLC11: 8/86\n",
            "Calculating errors of BNFS11: 9/86\n",
            "Calculating errors of BRCO11: 10/86\n",
            "Calculating errors of BTAL11: 11/86\n",
            "Calculating errors of BTLG11: 12/86\n",
            "Calculating errors of BTRA11: 13/86\n",
            "Calculating errors of BTWR11: 14/86\n",
            "Calculating errors of CBOP11: 15/86\n",
            "Calculating errors of CEOC11: 16/86\n",
            "Calculating errors of CNES11: 17/86\n",
            "Calculating errors of CXCE11B: 18/86\n",
            "Calculating errors of CXCO11: 19/86\n",
            "Calculating errors of DRIT11B: 20/86\n",
            "Calculating errors of EDFO11B: 21/86\n",
            "Calculating errors of EDGA11: 22/86\n",
            "Calculating errors of EURO11: 23/86\n",
            "Calculating errors of FAED11: 24/86\n",
            "Calculating errors of FCFL11: 25/86\n",
            "Calculating errors of FIGS11: 26/86\n",
            "Calculating errors of FIIP11B: 27/86\n",
            "Calculating errors of FPAB11: 28/86\n",
            "Calculating errors of GESE11B: 29/86\n",
            "Calculating errors of GGRC11: 30/86\n",
            "Calculating errors of GTWR11: 31/86\n",
            "Calculating errors of HAAA11: 32/86\n",
            "Calculating errors of HCTR11: 33/86\n",
            "Calculating errors of HGBS11: 34/86\n",
            "Calculating errors of HGLG11: 35/86\n",
            "Calculating errors of HGPO11: 36/86\n",
            "Calculating errors of HGRE11: 37/86\n",
            "Calculating errors of HLOG11: 38/86\n",
            "Calculating errors of HPDP11: 39/86\n",
            "Calculating errors of HSLG11: 40/86\n",
            "Calculating errors of HSML11: 41/86\n",
            "Calculating errors of HUSC11: 42/86\n",
            "Calculating errors of JFLL11: 43/86\n",
            "Calculating errors of LASC11: 44/86\n",
            "Calculating errors of LGCP11: 45/86\n",
            "Calculating errors of LUGG11: 46/86\n",
            "Calculating errors of MALL11: 47/86\n"
          ]
        }
      ],
      "source": [
        "pred_column = 'CloseChange'\n",
        "possibilities = get_possibilities(pred_column, (['Close','DividendYield','DividendsChange','DividendYieldChange','SectorDividendsChangeMean','Selic','IPCA','IGPM','IFIX','SectorCloseChangeMean']))\n",
        "\n",
        "params = {\"n_estimators\" : 4000, \"early_stopping_rounds\" : 100, \"learning_rate\": 0.1, \"max_depth\": 6,\"min_child_weight\" : 1,\"colsample_bytree\" : 0.4}\n",
        "\n",
        "errors, possibilities = prepare_errors_array(possibilities, 'Xgboost')\n",
        "\n",
        "for j, array_possibility in enumerate(possibilities):\n",
        "\n",
        "    rmse_array, string_possibility = [], \", \".join(array_possibility)\n",
        "\n",
        "    if string_possibility not in existent_experiments:\n",
        "\n",
        "        print('Calculating \"'+ string_possibility + '\": ' + str(j+1) + '/' + str(len(possibilities)))\n",
        "\n",
        "        for i, fundo in enumerate(df_history[\"Ticker\"].unique()):\n",
        "\n",
        "            print(\"Calculating errors of \"+ fundo + \": \" + str(i+1) + \"/\" + str(len(df_history[\"Ticker\"].unique())))\n",
        "\n",
        "            prediction, rmse = machinelearn_predict(df_history, fundo, pred_column, array_possibility, params, janela_treino)\n",
        "            rmse_array.append(rmse)\n",
        "\n",
        "        upload_rmse(pred_column, string_possibility, 'Xgboost', sectors, rmse_array, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AgWoNWx5XIN"
      },
      "outputs": [],
      "source": [
        "df_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PWlhe1C5Xuo"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "df_history.to_csv('df_history.csv', encoding = 'utf-8-sig') \n",
        "files.download('df_history.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z98tydUc5fAQ"
      },
      "outputs": [],
      "source": [
        "df_history_daily.to_csv('df_history_daily.csv', encoding = 'utf-8-sig') \n",
        "files.download('df_history_daily.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KiywEkwhdHss"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}